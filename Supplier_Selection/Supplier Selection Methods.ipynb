{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplier Ranking Methods (Multi-criteria Decision-Making or MCDM)\n",
    "Prepared by: Nickolas Freeman, Ph.D.\n",
    "\n",
    "This notebook presents techniques for ranking suppliers. These techniques are useful for identifying a subset of candidate suppliers from a potentially large set of available suppliers. The difficulty in such selection tasks is that oftentimes there are several competing criteria that we would like to use for evaluation. For example, buying firms care about both cost and quality. However, it may be impossible to find a supplier that offers exceptional performance with respect to both criteria because the two criteria may be negatively correlated (e.g., the supplier is able to offer products at a lower cost because they do not enforce very stringent quality controls). \n",
    "\n",
    "Another complicating factor is that it is common for a buying firm to be interested in both quantitative and qualitative criteria. However, comparing scores for quantitative and qualitative aspects is difficult. For example, how much of a cost increase is improved technical support or sustainability efforts worth? The approaches we consider in this notebook assign a quantitative score to subjective criteria and employ relatively simple weighting techniques to determine overall scores. A hyperlinked table of contents follows.\n",
    "\n",
    "<a id=\"Table_of_Contents\"> </a>\n",
    "# Table of Contents\n",
    "1. [Data Preparation](#Data_Preparation)<br>\n",
    "2. [Technique 1: Weighted Sum Method](#Weighted_Sum_Method)<br>\n",
    "3. [Technique 2: Weighted Product Method](#Weighted_Product_Method)<br>\n",
    "4. [Technique 3: TOPSIS Method](#TOPSIS_Method)<br>\n",
    "5. [The mcdm_functions Module](#mcdm_functions)<br>\n",
    "6. [Weight Determination via Analytic Hierarchy Process (AHP)](#AHP)<br>\n",
    "    \n",
    "The following code block imports some packages that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:,.3f}'.format)\n",
    "import seaborn as sns\n",
    "sns.set_style(style = 'whitegrid')\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, we will use a dataset that includes data for several trucking service suppliers. The data includes qualitative and quantitative factors that we will use to rank the suppliers. The following code block 1) reads the data into a Pandas DataFrame named `tr_data`, 2) prints the number of rows and columns using the `shape` of the DataFrame, and 3) prints the first five rows of the data using the `head` method of the DataFrame. \n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = pd.read_csv('tr_supplier_data.csv')\n",
    "\n",
    "rows, columns = tr_data.shape\n",
    "print(f'The data has {rows} rows and {columns} columns.')\n",
    "\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_Preparation\"> </a>\n",
    "# Data Preparation\n",
    "\n",
    "\n",
    "Assume that we want to use the following criteria to rank the suppliers:\n",
    "\n",
    "1. we only want to consider companies that are primarily focused on specialized long distance trucking,\n",
    "2. whether the supplier is open on Saturdays (yes is better),\n",
    "3. whether the supplier is open on Sundays (yes is better),\n",
    "4. the suppliers credit score (higher is better),\n",
    "5. the size of the supplier (higher is better),\n",
    "6. the suppliers location (Tuscaloosa is preferred, AL is second preference)\n",
    "\n",
    "The following code block allows us to inspect the primary NAICS for all entries in the data. Note tha the description for NAICS code 484230 most closely matches what we desire.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_columns = ['Primary NAICS', 'Primary NAICS Description']\n",
    "agg_dict = {'State': 'count'}\n",
    "\n",
    "tr_data.groupby(agg_columns).agg(agg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block restricts the data to only include NAICS code 484230.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_naics = [484230]\n",
    "\n",
    "tr_data = tr_data[tr_data['Primary NAICS'].isin(relevant_naics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider where the suppliers are open on Saturday or Sunday. The following code block prints the unique values in the `Saturday Open` and `Sunday Open` columns.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The unique values in the Saturday Open column are {tr_data[\"Saturday Open\"].unique()}')\n",
    "print(f'The unique values in the Sunday Open column are {tr_data[\"Sunday Open\"].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we cannot determine whether a supplier with a `nan` value in the columns is open or not, we will interpret the values `Closed` and `nan` as indicating that a supplier is not open on a given day. The following code block uses this decision rule to assign a score of 1 to suppliers that are open on Saturday(Sunday) and a score of 0.1 to suppliers that are not open on Saturday(Sunday).\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonopen_values = [np.nan, 'Closed']\n",
    "\n",
    "saturday_closed_mask = tr_data['Saturday Open'].isin(nonopen_values)\n",
    "tr_data['Open Saturday'] = 1\n",
    "tr_data.loc[saturday_closed_mask, 'Open Saturday'] = 0.1\n",
    "\n",
    "sunday_closed_mask = tr_data['Sunday Open'].isin(nonopen_values)\n",
    "tr_data['Open Sunday'] = 1\n",
    "tr_data.loc[sunday_closed_mask, 'Open Sunday'] = 0.1\n",
    "\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider the credit score for each supplier. The following code block prints the unique values that occur in the `Credit Score Alpha` column.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Credit Score Alpha'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a dictionary object that maps each of the observed credit score values to a value in the interval $[0,1]$, where higher values are better.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_score_mapper = {'A+': 1,\n",
    "                       'A': 0.95,\n",
    "                       'B+': 0.7,\n",
    "                       'B': 0.5,\n",
    "                       'C': 0,\n",
    "                       'C+': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block use ths `map` method to lookup the values in the `Credit Score Alpha` in the `credit_score_mapper` object and assign the appropriate values back to the DataFrame.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Credit Score Num'] = tr_data['Credit Score Alpha'].map(credit_score_mapper)\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now consider the size of the supplier's business. We asume that we are interested in working with larger, presumably more established companies. If you look at the columns, there are several columns that we might be able to use as a proxy for size. In particular, we might consider `Location Employee Size Actual`, `Location Sales Volume Actual`, and maybe even `Square Footage`. Looking at the data, one will observe that there is a high degree of correlation among the values in these columns. We will use `Location Sales Volume Actual`as a proxy for the size of the supplier's buisness. The following code block prints the column. Note that the reported `dtype` is `object`. This is telling is that pandas is interpreting the values in the column as strings.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Location Sales Volume Actual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the `Location Sales Volume Actual` column are being interpreted as strings because of the `$` signs and commas. The following code block shows how we can use *string methods* on a pandas column to remove these characters.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Location Sales Volume Actual'] = tr_data['Location Sales Volume Actual'].str.replace('$', '')\n",
    "tr_data['Location Sales Volume Actual'] = tr_data['Location Sales Volume Actual'].str.replace(',', '')\n",
    "tr_data['Location Sales Volume Actual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block converts the type of data stored in the `Location Sales Volume Actual` column to a numeric type.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Location Sales Volume Actual'] = pd.to_numeric(tr_data['Location Sales Volume Actual'])\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing we wanted to consider was the supplier's location. Specifically, we want to enforce a rating that assigns the highest weight to suppliers in Tuscaloosa, the second highest weight to suppliers in Alabama, and a base weight to all other suppliers. To apply such a rule, we will construct a dictionary of zip codes that contains the appropriate values. We will then map this dictionary back onto the data as we did when considering supplier credit scores. The first step in doing this is to determine all of the unique (city, state, zip code) tuples in the data. This is accomplished in the following code block. The first five tuples are printed.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_columns = ['City', 'State', 'ZIP Code']\n",
    "agg_dict = {'Company Name': 'count'}\n",
    "\n",
    "city_state_pairs = tr_data.groupby(agg_columns).agg(agg_dict).index.tolist()\n",
    "city_state_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses basic flow control to create the dictionary that will be used for mapping. Some values are printed.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_code_mapper = {}\n",
    "for city, state, zip_code in city_state_pairs:\n",
    "    if ((city == 'Tuscaloosa') and (state == 'AL')):\n",
    "        zip_code_mapper[zip_code] = 1\n",
    "    elif state == 'AL':\n",
    "        zip_code_mapper[zip_code] = 0.8\n",
    "    else:\n",
    "        zip_code_mapper[zip_code] = 0.5\n",
    "        \n",
    "print(f'The value for 35405 is {zip_code_mapper[35405]}')\n",
    "print(f'The value for 37705 is {zip_code_mapper[37705]}')\n",
    "print(f'The value for 35210 is {zip_code_mapper[35210]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block maps the values onto the data and prints the first five rows.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Zip Code Score'] = tr_data['ZIP Code'].map(zip_code_mapper)\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have columns in our data that we can use to evaluate the available suppliers on the desired dimensions. The following code block uses the modifed `tr_data` object to construct a new DataFrame, `clean_data`, that contains the minimal amount of columns we need for the ranking.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['Company Name', \n",
    "                   'City', \n",
    "                   'State', \n",
    "                   'ZIP Code',\n",
    "                   'Open Saturday',\n",
    "                   'Open Sunday', \n",
    "                   'Credit Score Num', \n",
    "                   'Zip Code Score', \n",
    "                   'Location Sales Volume Actual']\n",
    "clean_data = tr_data[columns_to_keep].copy()\n",
    "clean_data = clean_data.reset_index(drop = True)\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing that we need to consider is normalizing the data. In particular, notice that the values in the `Location Sales Volume Actual` column are significantly larger than those in the `Open Saturday`, `Open Sunday`, `Credit Score Num`, and `Zip Code Score` columns. Depending on the raking method we use, this could cause the rankings to be overly biased by the sales volume. In general, it is good to normalize all values that we will be using for the ranking so that they range from zero to one. The following code block shows how we can use the `MinMaxScaler` from the `sklearn.preprocessing` modules to rescale the values.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sales volume data as a numpy array\n",
    "sales_volume_array = clean_data['Location Sales Volume Actual'].values\n",
    "\n",
    "# Reshape the array so that is multi-dimensional with one column\n",
    "# and as many rows as needed\n",
    "sales_volume_array = sales_volume_array.reshape(-1, 1)\n",
    "\n",
    "# Normalize the array\n",
    "clean_data['Location Sales Volume Actual'] = preprocessing.MinMaxScaler().fit_transform(sales_volume_array)\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data prepared, we will now define the weights that we will use for supplier ranking. These weights indicate the relative importance of the various attributes for each supplier. We will deine these weights using the dictionary that is specified in the following code block.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'Open Saturday': 3,\n",
    "           'Open Sunday': 2, \n",
    "           'Credit Score Num': 9, \n",
    "           'Zip Code Score': 7,\n",
    "           'Location Sales Volume Actual': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was true with the values we will use for ranking the suppliers, it is also common practice to normalize the weights. We normalize the weights so that they sum to one by dividing each weight value by the sum of the weights.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {key: value/sum(weights.values()) for key, value in weights.items()}\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Weighted_Sum_Method\"> </a>\n",
    "# Technique 1: Weighted Sum Method\n",
    "\n",
    "The weighted sum approach we are going to look at is arguably the best known and simplest method for evaluating a set of alternatives according to a number of criteria. \n",
    "\n",
    ">[The weighted sum method determines a] weighted point [estimate for each alternative], which consider attributes that are weighted by the buyer. The weight for each attribute is then multiplied by the performance score that is assigned. Finally, these products are totaled to determine final rating for each supplier. Typically this system is designed to utilize quantitative measurements. The advantages of the weighted point method include the ability for the organization to include numerous evaluation factors and assign them weights according to the organization’s needs. The subjective factors on the evaluation are minimized. The major limitation of this approach is that it is difficult to effectively take qualitative evaluation criteria into consideration.\n",
    ">\n",
    "> - Khaled, A.A., Paul, S. K., Ripon Kumar Chakrabortty, M., & Ayuby, S. (2011). Selection of suppliers through different multi-criteria decision making techniques. Global Journal of Management and Business Research, 11(4).\n",
    "\n",
    "Assume that we want to evaluate $m$ alternatives (suppliers) on $n$ criteria and that higher values of all criteria are better. Let $w_{j}$ denote the relative weight of criterion $j$ and $a_{ij}$ denote the score of alternative $i$ in terms of criterion $j$. Using the described notation, the weighted sum score for alternative $i$ is given by:\n",
    "\n",
    "$$\\sum_{j=1}^{n}w_{j}a_{ij},\\mbox{ for }i=1,2,3,\\ldots,m.$$\n",
    "\n",
    "The following code block shows how we can perform the weighted sum evaluation using our data and weights.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty pandas series with the same index as our data\n",
    "# and all values initialized to zero\n",
    "temp = pd.Series(index = clean_data.index, data = 0)\n",
    "\n",
    "# for each key in our weight dictionary\n",
    "for key, weight in weights.items():\n",
    "    \n",
    "    # if the key corresponds to a column in our data\n",
    "    if key in clean_data.columns:\n",
    "        \n",
    "        # increment the value of the series by the product\n",
    "        # of the weight times the column values\n",
    "        temp += clean_data[key]*weight\n",
    "        \n",
    "    # if the key does not correspond to a column\n",
    "    else:\n",
    "        \n",
    "        # pass\n",
    "        continue\n",
    "\n",
    "# convert the series to a list\n",
    "temp = temp.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block prints the first five weight values.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block specifies a function that computes the weighted sum calculations.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_sum(data, weights_dict):\n",
    "    \"\"\"\n",
    "    Computes weighted sum for specified columns\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    data : a pandas DataFrame object that contains the columns to be weighted and the\n",
    "    normalized scores for each alternative\n",
    "    \n",
    "    weights_dict: a dictionary containing the columns to be included in the weighted sum as keys,\n",
    "    and the associated values as values\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    A list specifying the weighted sum calculations\n",
    "           \n",
    "    \"\"\"   \n",
    "\n",
    "    temp = pd.Series(index = data.index, data = 0)\n",
    "\n",
    "    for key, weight in weights_dict.items():\n",
    "        if key in data.columns:\n",
    "            temp += data[key]*weight\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return temp.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how the function may be used to create a column in our `clean_data` object that specifies the weighted sum scores.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['WS'] = compute_weighted_sum(clean_data, weights)\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Weighted_Product_Method\"> </a>\n",
    "# Technique 2: Weighted Product Method\n",
    "\n",
    "We will now consider the weighted product method for ranking alternatives. Using the notation we defined earlier, the weighted product score for alternative $A_{i}$ is given by:\n",
    "\n",
    "$$\\prod_{j=1}^{n}(a_{ij})^{w_{j}},\\mbox{ for }i=1,2,3,\\ldots,m.$$\n",
    "\n",
    "The following code block defines a function that determines the weighted product ranking for a set of alternatives. A key thing to note is that instead of initializing all values of the series to zero, we must initialize them to one for the weighted product to work.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_product(data, weights_dict):\n",
    "    \"\"\"\n",
    "    Computes weighted product for specified columns\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    data : a pandas DataFrame object that contains the columns to be weighted and the\n",
    "    normalized scores for each alternative\n",
    "    \n",
    "    weights_dict: a dictionary containing the columns to be included in the weighted product as keys,\n",
    "    and the associated values as values\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    A list specifying the weighted product calculations\n",
    "           \n",
    "    \"\"\"   \n",
    "    \n",
    "    temp = pd.Series(index = data.index, data = 1)\n",
    "    for key, weight in weights_dict.items():\n",
    "        if key in data.columns:\n",
    "            temp *= (data[key]**weight)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return temp.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how the function may be used to create a column in our `clean_data` object that specifies the weighted sum scores.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['WP'] = compute_weighted_product(clean_data, weights)\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block plots the weighted sum and weighted product scores using a scatterplot. What do you think is the cause for the weighted product evaluations that equal zero?\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (6, 6))\n",
    "\n",
    "sns.scatterplot(x = 'WS',\n",
    "                y = 'WP',\n",
    "                s = 100,\n",
    "                data = clean_data)\n",
    "\n",
    "ax.set_xlim(-0.02, 1.02)\n",
    "ax.set_ylim(-0.02, 1.02)\n",
    "\n",
    "diag_line, = ax.plot(ax.get_xlim(), \n",
    "                     ax.get_ylim(), \n",
    "                     ls=\"--\", \n",
    "                     c=\".3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TOPSIS_Method\"> </a>\n",
    "# Technique 3: TOPSIS\n",
    "\n",
    "We will now consider the TOPSIS technique. TOPSIS stands for <b>T</b>echnique for <b>O</b>rder of <b>P</b>reference by <b>S</b>imilarity to <b>I</b>deal <b>S</b>olution. From https://en.wikipedia.org/wiki/TOPSIS (accessed on 2/8/18):\n",
    "\n",
    "> TOPSIS is a multi-criteria decision analysis method, which was originally developed by Hwang and Yoon in 1981 with further developments by Yoon in 1987, and Hwang, Lai and Liu in 1993. TOPSIS is based on the concept that the chosen alternative should have the shortest geometric distance from the positive ideal solution (PIS) and the longest geometric distance from the negative ideal solution (NIS). It is a method of compensatory aggregation that compares a set of alternatives by identifying weights for each criterion, normalising scores for each criterion and calculating the geometric distance between each alternative and the ideal alternative, which is the best score in each criterion. An assumption of TOPSIS is that the criteria are monotonically increasing or decreasing. Normalisation is usually required as the parameters or criteria are often of incongruous dimensions in multi-criteria problems. Compensatory methods such as TOPSIS allow trade-offs between criteria, where a poor result in one criterion can be negated by a good result in another criterion.\n",
    "\n",
    "The following code cell generates an interactive plot that illustrates the location of the *positive ideal solution* (PIS) and *negative ideal solution* (NIS) for a dataset with two criterion. Note that this representation assumes that higher values of each criteria are better.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(20)*9+1\n",
    "y = np.random.rand(20)*9+1\n",
    "\n",
    "min_x = min(x)\n",
    "max_x = max(x)\n",
    "min_y = min(y)\n",
    "max_y = max(y)\n",
    "\n",
    "show_IS_list = [False, True]\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact(show_IS = show_IS_list)\n",
    "\n",
    "def interactive_plot(show_IS = show_IS_list[0]):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (6, 6))\n",
    "\n",
    "    ax.scatter(x, y, s = 100)\n",
    "    ax.set_xlim(0,11)\n",
    "    ax.set_ylim(0,11)\n",
    "    ax.set_xlabel('Criterion x', fontsize = 16)\n",
    "    ax.set_ylabel('Criterion y', fontsize = 16)\n",
    "    if show_IS:\n",
    "        ax.axvline(max_x, linestyle = '--', color = 'k')\n",
    "        ax.axhline(min_y, linestyle = '--', color = 'k')\n",
    "        ax.axhline(max_y, linestyle = '--', color = 'k')\n",
    "        ax.axvline(min_x, linestyle = '--', color = 'k')\n",
    "        ax.axvline(max_x, linestyle = '--', color = 'k')\n",
    "        ax.scatter(min_x, min_y, color = 'k', s = 200)\n",
    "        ax.scatter(max_x, max_y, color = 'k', s = 200)\n",
    "        ax.annotate('NIS', xy = (min_x + 1, min_y +1), fontsize = 16)\n",
    "        ax.annotate('PIS', xy = (max_x + 0.5, max_y - 1.4), fontsize = 16)\n",
    "        ax.arrow(min_x + 1, min_y +1, -0.6, -0.6, head_width=0.3, head_length=0.3, fc='k', ec='k')\n",
    "        ax.arrow(max_x + 0.5, max_y - 1, -0.3, 0.5, head_width=0.3, head_length=0.3, fc='k', ec='k')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TOPSIS process is carried out as follows (from https://en.wikipedia.org/wiki/TOPSIS (accessed on 2/8/18)):\n",
    "\n",
    "> #### Step 1:\n",
    ">Create an evaluation matrix consisting of m alternatives and n criteria, with the intersection of each alternative and criteria given as $\\displaystyle x_{ij}$, we therefore have a matrix $\\displaystyle (x_{ij})_{m\\times n}$.\n",
    "> \n",
    "> #### Step 2:\n",
    "> The matrix $\\displaystyle (x_{ij})_{m\\times n}$ is then normalized to form the matrix $R = \\displaystyle (r_{ij})_{m\\times n}$, using the normalization method $\\displaystyle r_{ij} = \\frac{x_{ij}}{\\sqrt{\\sum_{i=1}^{m}x^{2}_{ij}}}, i = 1,2,\\ldots,m,~j = 1,2,\\ldots,n$.\n",
    ">\n",
    "> #### Step 3:\n",
    "> Calculate the weighted normalized decision matrix $\\displaystyle t_{ij}=r_{ij}\\cdot w_{j},~i=1,2,...,m,~j=1,2,...,n$, where $\\displaystyle w_{j}=W_{j}/\\sum _{j=1}^{n}W_{j},~j=1,2,...,n$ so that $\\displaystyle \\sum _{j=1}^{n}w_{j}=1$, and $W_{j}$ is the original weight given to indicator $\\displaystyle v_{j},~j=i,2,\\ldots,n$.\n",
    ">\n",
    "> #### Step 4: \n",
    "> Determine the worst alternative $\\displaystyle (A_{w})$ and the best alternative $\\displaystyle (A_{b})$:\n",
    ">\n",
    "> $$A_{w}=\\{\\langle max(t_{{ij}}|i=1,2,...,m)|j\\in J_{-}\\rangle ,\\langle min(t_{{ij}}|i=1,2,...,m)|j\\in J_{+}\\rangle \\rbrace \\equiv \\{t_{{wj}}|j=1,2,...,n\\rbrace,$$\n",
    ">\n",
    "> $$\\displaystyle A_{b}=\\{\\langle min(t_{ij}|i=1,2,...,m)|j\\in J_{-}\\rangle ,\\langle max(t_{ij}|i=1,2,...,m)|j\\in J_{+}\\rangle \\rbrace \\equiv \\{t_{bj}|j=1,2,...,n\\},$$\n",
    ">\n",
    "> where,\n",
    "$\\displaystyle J_{+}=\\{j=1,2,...,n|j\\}$ associated with the criteria having a positive impact, and\n",
    "$\\displaystyle J_{-}=\\{j=1,2,...,n|j\\}$ associated with the criteria having a negative impact.\n",
    ">\n",
    "> #### Step 5:\n",
    "> Calculate the L2-distance between the target alternative $\\displaystyle i$ and the worst condition $\\displaystyle A_{w}$,\n",
    ">\n",
    "> $$\\displaystyle d_{iw}={\\sqrt {\\sum _{j=1}^{n}(t_{ij}-t_{wj})^{2}}},i=1,2,...,m,$$\n",
    ">\n",
    "> and the distance between the alternative $\\displaystyle i$ and the best condition $\\displaystyle A_{b}$,\n",
    ">\n",
    "> $$\\displaystyle d_{ib}={\\sqrt {\\sum _{j=1}^{n}(t_{ij}-t_{bj})^{2}}},i=1,2,...,m,$$\n",
    ">\n",
    "> where $\\displaystyle d_{iw}$ and $\\displaystyle d_{ib}$ are L2-norm distances from the target alternative $\\displaystyle i$ to the worst and best conditions, respectively.\n",
    ">\n",
    "> #### Step 6\n",
    "> Calculate the similarity to the worst condition:\n",
    ">\n",
    "> $$\\displaystyle s_{iw}=d_{iw}/(d_{iw}+d_{ib}),0\\leq s_{iw}\\leq 1,i=1,2,...,m.$$\n",
    ">\n",
    "> $\\displaystyle s_{iw}=1$ if and only if the alternative solution has the best condition; and\n",
    ">\n",
    "> $\\displaystyle s_{iw}=0$ if and only if the alternative solution has the worst condition.\n",
    ">\n",
    "> #### Step 7\n",
    "> Rank the alternatives according to $\\displaystyle s_{iw}(i=1,2,...,m)$.\n",
    "\n",
    "The following code block performs these steps on our data.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0 - create copy of data and get evaluation columns\n",
    "temp_df = clean_data.copy()\n",
    "criteria_columns = list(weights.keys())\n",
    "criteria_columns = [col for col in criteria_columns if col in temp_df.columns]\n",
    "\n",
    "#Step 1\n",
    "evaluation_matrix = temp_df[criteria_columns].values\n",
    "\n",
    "#Step 2\n",
    "squared_evaluation_matrix = evaluation_matrix**2\n",
    "normalized_evaluation_matrix = evaluation_matrix/np.sqrt(np.sum(squared_evaluation_matrix, axis=0))\n",
    "\n",
    "#Step 3\n",
    "weight_values = list(weights.values())\n",
    "weight_values = np.array(weight_values)\n",
    "weight_values = weight_values/weight_values.sum()\n",
    "weighted_matrix = normalized_evaluation_matrix * weight_values\n",
    "\n",
    "#Step 4\n",
    "PIS = np.max(weighted_matrix, axis=0)\n",
    "NIS = np.min(weighted_matrix, axis=0)\n",
    "\n",
    "#Step 5\n",
    "intermediate = (weighted_matrix - PIS)**2\n",
    "Dev_Best = np.sqrt(intermediate.sum(axis = 1))\n",
    "\n",
    "intermediate = (weighted_matrix - NIS)**2\n",
    "Dev_Worst = np.sqrt(intermediate.sum(axis = 1))\n",
    "\n",
    "#Step 6\n",
    "Closeness = Dev_Worst/(Dev_Best+Dev_Worst)\n",
    "\n",
    "temp_df['TOPSIS Score'] = Closeness.tolist()\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block specifies a function for computing the TOPSIS score.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TOPSIS(data, weights_dict):\n",
    "    \"\"\"\n",
    "    Computes TOPSIS score for specified columns\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    data : a pandas DataFrame object that contains the columns to be weighted and the\n",
    "    normalized scores for each alternative\n",
    "    \n",
    "    weights_dict: a dictionary containing the columns to be included in the weighted product as keys,\n",
    "    and the associated values as values\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    A list specifying the TOPSIS calculations\n",
    "           \n",
    "    \"\"\"   \n",
    "    \n",
    "    #Step 0 \n",
    "    temp_df = data.copy()\n",
    "    criteria_columns = list(weights_dict.keys())\n",
    "    criteria_columns = [col for col in criteria_columns if col in temp_df.columns]\n",
    "\n",
    "    #Step 1\n",
    "    evaluation_matrix = temp_df[criteria_columns].values\n",
    "\n",
    "    #Step 2\n",
    "    squared_evaluation_matrix = evaluation_matrix**2\n",
    "    normalized_evaluation_matrix = evaluation_matrix/np.sqrt(np.sum(squared_evaluation_matrix, axis=0))\n",
    "\n",
    "    #Step 3\n",
    "    weight_values = list(weights_dict.values())\n",
    "    weight_values = np.array(weight_values)\n",
    "    weight_values = weight_values/weight_values.sum()\n",
    "    weighted_matrix = normalized_evaluation_matrix * weight_values\n",
    "\n",
    "    #Step 4\n",
    "    PIS = np.max(weighted_matrix, axis=0)\n",
    "    NIS = np.min(weighted_matrix, axis=0)\n",
    "\n",
    "    #Step 5\n",
    "    intermediate = (weighted_matrix - PIS)**2\n",
    "    Dev_Best = np.sqrt(intermediate.sum(axis = 1))\n",
    "\n",
    "    intermediate = (weighted_matrix - NIS)**2\n",
    "    Dev_Worst = np.sqrt(intermediate.sum(axis = 1))\n",
    "\n",
    "    #Step 6\n",
    "    Closeness = Dev_Worst/(Dev_Best+Dev_Worst)\n",
    "\n",
    "    #Step 7\n",
    "    return Closeness.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folowing code block applies our TOPSIS function to the `clean_data` object.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['TOPSIS'] = compute_TOPSIS(clean_data, weights)\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block generates a plot comparing the three ranking methods.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "model_list = ['WS', 'WP', 'TOPSIS']\n",
    "model_combinations = list(itertools.combinations(model_list, r = 2))\n",
    "\n",
    "fig, ax = plt.subplots(1, len(model_combinations), figsize = (15, 4))\n",
    "\n",
    "for ((model1, model2), current_ax) in zip(model_combinations, ax):\n",
    "    \n",
    "    sns.scatterplot(x = model1,\n",
    "                    y = model2,\n",
    "                    s = 100,\n",
    "                    ax = current_ax,\n",
    "                    data = clean_data)\n",
    "\n",
    "    current_ax.set_xlim(-0.1, 1.1)\n",
    "    current_ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "    diag_line, = current_ax.plot(current_ax.get_xlim(), \n",
    "                                 current_ax.get_ylim(), \n",
    "                                 ls=\"--\", \n",
    "                                 c=\".3\")\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mcdm_functions\"> </a>\n",
    "# The mcdm_functions module\n",
    "\n",
    "Up to this point, we have built functions in the current notebook to apply our ranking methods. In practice, we probably would like to define the functions in a separate file that we can import like `pandas` or `numpy`. The following code block imports such a `module`, which should be saved in the current working directory (the directory where this notebook resides).\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mcdm_functions as mcdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate the functionality available in the module using a fresh copy of the data we previously cleaned.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['Company Name', \n",
    "                   'City', \n",
    "                   'State', \n",
    "                   'ZIP Code',\n",
    "                   'Open Saturday',\n",
    "                   'Open Sunday', \n",
    "                   'Credit Score Num', \n",
    "                   'Zip Code Score', \n",
    "                   'Location Sales Volume Actual']\n",
    "\n",
    "clean_data = tr_data[columns_to_keep].copy()\n",
    "clean_data = clean_data.reset_index(drop = True)\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will use the same weights. The dictionary of weights is recreated below.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'Open Saturday': 3,\n",
    "           'Open Sunday': 2, \n",
    "           'Credit Score Num': 9, \n",
    "           'Zip Code Score': 7,\n",
    "           'Location Sales Volume Actual': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the imported module, which we gave the alias `mcdm`, there is a function named `normalize_weights_dictionary`. This function accepts a dictionary of weights as an input, and returns a copy of the dictionary with the weight values normalized.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = mcdm.normalize_weights_dictionary(weights)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in our *refreshed* version of the data, the `Location Sales Volume Actual` column is not normalized. The `mcdm` module contains a function named `normalize_array` that will uses the `MinMaxScaler` from scikit-learn to normalize a single array of numbers proved as a list, a pandas Series, or a numpy array.  \n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['Location Sales Volume Actual'] = mcdm.normalize_array(clean_data['Location Sales Volume Actual'])\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mcdm` module three functions that allow us to compute the weighted sum (`compute_weighted_sum`), the weighted product (`compute_weighted_product`), and the TOPSIS (`compute_TOPSIS`) scores for a provided DataFrame. The values in the passed DataFrame should be normalized before running any of these functions. The functions also accept a dictionary of weights, where the `keys` match columns in the DataFrame. The weight `values` in the dictionary should be normalized.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['WS'] = mcdm.compute_weighted_sum(clean_data, weights)\n",
    "clean_data['WP'] = mcdm.compute_weighted_product(clean_data, weights)\n",
    "clean_data['TOPSIS'] = mcdm.compute_TOPSIS(clean_data, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five rows of the data, after applying our functions, are shown below.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"AHP\"> </a>\n",
    "# Weight Determination via Analytic Hierarchy Process (AHP)\n",
    "\n",
    "We will now turn our attention to setting weights for a supplier ranking task. This can be a particularly challenging process because the fact that we are including an attribute means it is important to some degree. In this section, we will investigate a technique known as Analytics Heirarchy Process for setting weights. From https://en.wikipedia.org/wiki/Analytic_hierarchy_process (accessed 2/4/2019):\n",
    "\n",
    "> The analytic hierarchy process (AHP) is a structured technique for organizing and analyzing complex decisions, based on mathematics and psychology. It was developed by Thomas L. Saaty in the 1970s and has been extensively studied and refined since then.\n",
    ">\n",
    "> Rather than prescribing a \"correct\" decision, the AHP helps decision makers find one that best suits their goal and their understanding of the problem. It provides a comprehensive and rational framework for structuring a decision problem, for representing and quantifying its elements, for relating those elements to overall goals, and for evaluating alternative solutions.\n",
    ">\n",
    "> Decision makers systematically evaluate its various [attributes] by comparing them to each other two at a time, with respect to their impact on [a predetermined objective]. In making the comparisons, the decision makers can use concrete data about the elements, but they typically use their judgments about the elements' relative meaning and importance. It is the essence of the AHP that human judgments, and not just the underlying information, can be used in performing the evaluations.\n",
    ">\n",
    "> The AHP converts these evaluations to numerical values that can be processed and compared over the entire range of the problem. A numerical weight or priority is derived for each [attribute], allowing diverse and often incommensurable elements to be compared to one another in a rational and consistent way. This capability distinguishes the AHP from other decision making techniques.\n",
    ">\n",
    "> In the final step of the process, numerical priorities are calculated for each of the decision alternatives. These numbers represent the alternatives' relative ability to achieve the decision goal, so they allow a straightforward consideration of the various courses of action.\n",
    "\n",
    "The pairwise evaluations performed in an AHP analysis follow the rubric provided in the table that follows.\n",
    "\n",
    "<table border=\"1\" class=\"colwidths-given docutils\">\n",
    "<colgroup>\n",
    "<col width=\"20%\" />\n",
    "<col width=\"20%\" />\n",
    "<col width=\"60%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\" style=\"text-align:center\"><th class=\"head\">Intensity of Importance</th>\n",
    "<th class=\"head\" style=\"text-align:left\">Definition</th>\n",
    "<th class=\"head\" style=\"text-align:left\">Explanation</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">$\\frac{1}{9} \\approx 0.111$</td>\n",
    "    <td style=\"text-align:left\">Extreme Unimportance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is extremely less preferable than criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">$\\frac{1}{7}\\approx 0.143$</td>\n",
    "    <td style=\"text-align:left\">Very Strong Unimportance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is very strongly less preferable than criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">$\\frac{1}{5} = 0.2$</td>\n",
    "    <td style=\"text-align:left\">Strong Unimportance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is strongly less preferable than criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">$\\frac{1}{3}\\approx 0.333$</td>\n",
    "    <td style=\"text-align:left\">Moderate Unimportance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is moderately less preferable than criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">1</td>\n",
    "    <td style=\"text-align:left\">Equal Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is equally important as criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">3</td>\n",
    "    <td style=\"text-align:left\">Moderate Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is moderately preferable to criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">5</td>\n",
    "    <td style=\"text-align:left\">Strong Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is strongly preferable to criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">7</td>\n",
    "    <td style=\"text-align:left\">Very Strong Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is very strongly preferable to criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">9</td>\n",
    "    <td style=\"text-align:left\">Extreme Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is extremely preferable to criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "A challenging aspect of performing an AHP analysis in Python is collecting the evaluation data and ensuring everything is in the correct format. Thus, we will utilize an automated approach that will be demonstrated shortly. First, let's use a small dataset on cereal preferences to demonstrate how AHP works.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = [\n",
    "    [1, 5, 9],\n",
    "    [0.2, 1, 4],\n",
    "    [0.111, 0.25, 1]\n",
    "]\n",
    "\n",
    "index_vals = ['Lucky Charms', 'Trix', 'Frosted Flakes']\n",
    "\n",
    "cereal_data = pd.DataFrame(comparison_data, index = index_vals, columns = index_vals)\n",
    "cereal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `values` attribute of a pandas DataFrame to get the values as a numpy array.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cereal_scores = cereal_data.values\n",
    "cereal_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Calculate column sums for the evaluation matrix.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = np.sum(cereal_scores,axis=0)\n",
    "column_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Divide all scores in the evaluation matrix by the column sums.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_divided_by_sums = cereal_scores/column_sums\n",
    "scores_divided_by_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Create a prority vector by determining the average score in each row of the updated evaluation matrix.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_vector = np.average(scores_divided_by_sums, axis=1)\n",
    "priority_vector   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Approximate the maximum eigenvalue of the evaluation matrix by computing the inner product of the priority vector and the column sums.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eigenvalue = np.inner(priority_vector, column_sums)\n",
    "max_eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Compute the consistency index (CI) for the evaluation matrix using the formula:\n",
    "\n",
    "$$ CI = (\\lambda - n)/(n-1),$$\n",
    "\n",
    "where $\\lambda$ represents the maximum eigenvalue and $n$ represents the number of attributes.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_CI = (max_eigenvalue - len(priority_vector))/(len(priority_vector)-1)\n",
    "criteria_CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "Compute the consistency ratio (CR) using the formula:\n",
    "\n",
    "$$ CI/RI_{n},$$\n",
    "\n",
    "where $RI_{n}$ is given in the following table.\n",
    "\n",
    "<table border=\"1\" class=\"colwidths-given docutils\">\n",
    "<colgroup>\n",
    "<col width=\"50%\" />\n",
    "<col width=\"50%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\" style=\"text-align:center\"><th class=\"head\">$n$</th>\n",
    "<th class=\"head\" style=\"text-align:center\">$RI_{n}$</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">0</td>\n",
    "    <td style=\"text-align:center\">0</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">1</td>\n",
    "    <td style=\"text-align:center\">0</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">2</td>\n",
    "    <td style=\"text-align:center\">0</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">3</td>\n",
    "    <td style=\"text-align:center\">0.58</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">4</td>\n",
    "    <td style=\"text-align:center\">0.9</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">5</td>\n",
    "    <td style=\"text-align:center\">1.12</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">6</td>\n",
    "    <td style=\"text-align:center\">1.24</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">7</td>\n",
    "    <td style=\"text-align:center\">1.32</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">8</td>\n",
    "    <td style=\"text-align:center\">1.41</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">9</td>\n",
    "    <td style=\"text-align:center\">1.45</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">10</td>\n",
    "    <td style=\"text-align:center\">1.49</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "If $CR < 0.10$, the evaluations are considered to be consistent.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RI = [0, 0, 0, 0.58, 0.9, 1.12, 1.24, 1.32, 1.41, 1.45, 1.49] \n",
    "criteria_CR = np.round(criteria_CI/RI[len(priority_vector)], 5)\n",
    "criteria_CR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block reruns the example with a set of data that includes inconsistent preferences.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = [\n",
    "    [1, 5, 9],\n",
    "    [0.2, 1, 0.25],\n",
    "    [0.111, 4, 1]\n",
    "]\n",
    "\n",
    "index_vals = ['Lucky Charms', 'Trix', 'Frosted Flakes']\n",
    "\n",
    "cereal_data = pd.DataFrame(comparison_data, index = index_vals, columns = index_vals)\n",
    "\n",
    "cereal_scores = cereal_data.values\n",
    "\n",
    "column_sums = np.sum(cereal_scores, axis=0)\n",
    "\n",
    "scores_divided_by_sums = cereal_scores/column_sums\n",
    "\n",
    "priority_vector = np.average(scores_divided_by_sums, axis=1)\n",
    "\n",
    "max_eigenvalue = np.inner(priority_vector, column_sums)\n",
    "\n",
    "criteria_CI = (max_eigenvalue - len(priority_vector))/(len(priority_vector)-1)\n",
    "\n",
    "criteria_CR = np.round(criteria_CI/RI[len(priority_vector)], 5)\n",
    "\n",
    "print(f'The consistency ratio is {criteria_CR}')\n",
    "\n",
    "cereal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mcdm module includes a function named `create_ahp_rank_matrix_from_gui` that we will use to automate the application of AHP. The function expects a `list` of criteria that should be compared. Suppose that the following list provides our order of relative importance for the selected criteria.\n",
    "\n",
    "1. `Credit Score Num`\n",
    "2. `Location Sales Volume Actual`\n",
    "3. `Zip Code Score`\n",
    "4. `Open Saturday`\n",
    "5. `Open Sunday`\n",
    "\n",
    "Moreover, assume that we have the following preferences using the language of AHP.\n",
    "\n",
    "- `Credit Score Num` is **moderately more preferable** than `Location Sales Volume Actual`\n",
    "- `Credit Score Num` is **strongly more preferable** than `Zip Code Score`\n",
    "- `Credit Score Num` is **very strongly more preferable** than `Open Saturday`\n",
    "- `Credit Score Num` is **extremely more preferable** than `Open Sunday`\n",
    "- `Location Sales Volume Actual` is **moderately more preferable** than `Zip Code Score`\n",
    "- `Location Sales Volume Actual` is **strongly more preferable** than `Open Saturday`\n",
    "- `Location Sales Volume Actual` is **very strongly more preferable** than `Open Sunday`\n",
    "- `Zip Code Score` is **moderately more preferable** than `Open Saturday`\n",
    "- `Zip Code Score` is **strongly more preferable** than `Open Sunday`\n",
    "- `Open Saturday` is **moderately more preferable** than `Open Sunday`\n",
    "\n",
    "You may correctly suspect that we can just past the `keys` of our `weights` dictionary as a `list` to the `create_ahp_rank_matrix_from_gui` function. The following code block shows how to generate such a list.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(weights.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although passing such a list is perfectly fine, it is best to provide the criteria in the order of decreasing expected importance. Such a representation for our criteria follows.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_list = ['Credit Score Num',\n",
    " 'Location Sales Volume Actual',\n",
    " 'Zip Code Score',\n",
    " 'Open Saturday',\n",
    " 'Open Sunday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block applies the function.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = mcdm.create_ahp_rank_matrix_from_gui(criteria_list)\n",
    "\n",
    "weights = {'Credit Score Num': 0.5028194957704966,\n",
    "           'Location Sales Volume Actual': 0.2602315877866833,\n",
    "           'Zip Code Score': 0.1343504405731109,\n",
    "           'Open Saturday': 0.06777766684747813,\n",
    "           'Open Sunday': 0.03482080902223112}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply our weights to our data, overwriting the previous scores.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['WS'] = mcdm.compute_weighted_sum(clean_data, weights)\n",
    "clean_data['WP'] = mcdm.compute_weighted_product(clean_data, weights)\n",
    "clean_data['TOPSIS'] = mcdm.compute_TOPSIS(clean_data, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block generates scatterplots for the scores found by the three methods.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "model_list = ['WS', 'WP', 'TOPSIS']\n",
    "model_combinations = list(itertools.combinations(model_list, r = 2))\n",
    "\n",
    "fig, ax = plt.subplots(1, len(model_combinations), figsize = (15, 4))\n",
    "\n",
    "for ((model1, model2), current_ax) in zip(model_combinations, ax):\n",
    "    \n",
    "    sns.scatterplot(x = model1,\n",
    "                    y = model2,\n",
    "                    s = 100,\n",
    "                    ax = current_ax,\n",
    "                    data = clean_data)\n",
    "\n",
    "    current_ax.set_xlim(-0.1, 1.1)\n",
    "    current_ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "    diag_line, = current_ax.plot(current_ax.get_xlim(), \n",
    "                                 current_ax.get_ylim(), \n",
    "                                 ls=\"--\", \n",
    "                                 c=\".3\")\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
